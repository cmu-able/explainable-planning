{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_assignments_ctrl = pd.read_csv('../XPlanningEvaluation/resources/mobilerobot/study/prefalign/mturk/assignments/assignments_control_full.csv')\n",
    "df_assignments_expr = pd.read_csv('../XPlanningEvaluation/resources/mobilerobot/study/prefalign/mturk/assignments/assignments_experimental_full.csv')\n",
    "df_answers_ctrl = pd.read_csv('../XPlanningEvaluation/resources/mobilerobot/study/prefalign/mturk/assignments/answerKey_control.csv')\n",
    "df_answers_expr = pd.read_csv('../XPlanningEvaluation/resources/mobilerobot/study/prefalign/mturk/assignments/answerKey_experimental.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_assignments_ctrl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_assignments_expr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_answers_ctrl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_answers_expr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create columns for all data types for all questions\n",
    "# Columns are named in the format: question[i]-[data_type_name]\n",
    "def create_columns(data_types, num_questions):\n",
    "    columns = []\n",
    "    for i in range(num_questions):\n",
    "        for data_type in data_types:\n",
    "            columns.append('question' + str(i) + \"-\" + data_type)\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create columns for a single data type for all questions\n",
    "# Columns are named in the format: question[i]-[data_type_name]\n",
    "def create_columns_for_data_type(data_type, num_questions):\n",
    "    return ['question' + str(i) + '-' + data_type for i in range(num_questions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove any assignment (i.e., any row) that contains missing data of a given data type of any question\n",
    "def remove_assignments_with_missing_data(df_assignments, data_type, num_questions):\n",
    "    columns = create_columns_for_data_type(data_type, num_questions)\n",
    "    non_numeric_df_assignments_columns = df_assignments.select_dtypes(exclude=['int','float']).columns\n",
    "    non_numeric_columns = [column for column in columns if column in non_numeric_df_assignments_columns]\n",
    "    for column in non_numeric_columns:\n",
    "        df_assignments = df_assignments[df_assignments[column] != 'null']\n",
    "    return df_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def set_columns_type_to_numeric(df_assignments, data_type, num_questions):\n",
    "    columns = create_columns_for_data_type(data_type, num_questions)\n",
    "    df_assignments[columns] = df_assignments[columns].apply(pd.to_numeric)\n",
    "    return df_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_assignments_ctrl = remove_assignments_with_missing_data(df_assignments_ctrl, 'total-cost', 4)\n",
    "df_assignments_ctrl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_assignments_expr = remove_assignments_with_missing_data(df_assignments_expr, 'total-cost', 4)\n",
    "df_assignments_expr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Compute accuracy of total-cost (in %error) and answer (correct/incorrect) for each question, from each worker\n",
    "def compute_accuracy(df_assignments, df_answers):\n",
    "    # Accuracy of total-cost (%error) and answer (correct/incorrect) from workers\n",
    "    accuracy_columns = create_columns(['ref', 'total-cost', 'answer'], 4)\n",
    "    df_accuracy = pd.DataFrame(columns=accuracy_columns)\n",
    "    \n",
    "    # Data columns of interest for each question: ref, total-cost, answer\n",
    "    ref_columns = create_columns_for_data_type('ref', 4)\n",
    "    total_cost_columns = create_columns_for_data_type('total-cost', 4)\n",
    "    answer_columns = create_columns_for_data_type('answer', 4)\n",
    "    \n",
    "    df_assignments = set_columns_type_to_numeric(df_assignments, 'total-cost', 4)\n",
    "    \n",
    "    for index, row in df_assignments.iterrows():\n",
    "        # ref: question-mission[i]-agent[j] is shared between assignments and answerKey tables\n",
    "        df_answer_key = df_answers[df_answers.ref.isin(row[ref_columns])]\n",
    "\n",
    "        # Accuracy of total-cost and answer from workers\n",
    "        worker_acc_dict = {'HIT ID': row['HIT ID'], 'Worker ID': row['Worker ID']}\n",
    "\n",
    "        for i in range(4):\n",
    "            ref_column = ref_columns[i] # question[i]-ref\n",
    "            total_cost_column = total_cost_columns[i] # question[i]-total-cost\n",
    "            answer_column = answer_columns[i] # question[i]-answer\n",
    "\n",
    "            # total-cost and %error\n",
    "            worker_total_cost = row[total_cost_column]\n",
    "            correct_total_cost = df_answer_key[df_answer_key.ref == row[ref_column]].iloc[0]['total-cost']\n",
    "            total_cost_err = abs((correct_total_cost - worker_total_cost) / correct_total_cost)\n",
    "\n",
    "            # answer and correct/incorrect\n",
    "            worker_answer = row[answer_column]\n",
    "            correct_answer = df_answer_key[df_answer_key.ref == row[ref_column]].iloc[0]['answer']\n",
    "            answer_acc = 1 if worker_answer == correct_answer else 0\n",
    "\n",
    "            # Accuracy dict\n",
    "            worker_acc_dict[ref_column] = row[ref_column]\n",
    "            worker_acc_dict[total_cost_column] = total_cost_err\n",
    "            worker_acc_dict[answer_column] = answer_acc\n",
    "\n",
    "        df_accuracy = df_accuracy.append(worker_acc_dict, ignore_index=True)\n",
    "    return df_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_accuracy_ctrl = compute_accuracy(df_assignments_ctrl, df_answers_ctrl)\n",
    "df_accuracy_ctrl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_accuracy_expr = compute_accuracy(df_assignments_expr, df_answers_expr)\n",
    "df_accuracy_expr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_score(df_assignments, df_answers):\n",
    "    # Score of answer-confidence combination\n",
    "    score_columns = create_columns(['ref', 'confidence'], 4)\n",
    "    df_score = pd.DataFrame(columns=score_columns)\n",
    "\n",
    "    confidence_abs_scores = {'none': 0, 'slightly': 1, 'somewhat': 2, 'fairly': 3, 'completely': 4}\n",
    "    \n",
    "    # Data columns of interest for each question: ref, answer, confidence\n",
    "    ref_columns = create_columns_for_data_type('ref', 4)\n",
    "    answer_columns = create_columns_for_data_type('answer', 4)\n",
    "    confidence_columns = create_columns_for_data_type('confidence', 4)\n",
    "\n",
    "    for index, row in df_assignments.iterrows():\n",
    "        # ref: question-mission[i]-agent[j] is shared between assignments and answerKey tables\n",
    "        df_answer_key = df_answers[df_answers.ref.isin(row[ref_columns])]\n",
    "\n",
    "        # Score of answer-confidence combination\n",
    "        worker_score_dict = {'HIT ID': row['HIT ID'], 'Worker ID': row['Worker ID']}\n",
    "\n",
    "        for i in range(4):\n",
    "            ref_column = ref_columns[i] # question[i]-ref\n",
    "            answer_column = answer_columns[i] # question[i]-answer\n",
    "            confidence_column = confidence_columns[i] # question[i]-confidence\n",
    "\n",
    "            # answer and correct/incorrect\n",
    "            worker_answer = row[answer_column]\n",
    "            correct_answer = df_answer_key[df_answer_key.ref == row[ref_column]].iloc[0]['answer']\n",
    "            answer_acc = worker_answer == correct_answer\n",
    "\n",
    "            # confidence and score\n",
    "            worker_confidence = row[confidence_column]\n",
    "            score = confidence_abs_scores[worker_confidence] if answer_acc else -1 * confidence_abs_scores[worker_confidence]\n",
    "\n",
    "            # Score dict\n",
    "            worker_score_dict[ref_column] = row[ref_column]\n",
    "            worker_score_dict[confidence_column] = score\n",
    "\n",
    "        df_score = df_score.append(worker_score_dict, ignore_index=True)\n",
    "    return df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_score_ctrl = compute_score(df_assignments_ctrl, df_answers_ctrl)\n",
    "df_score_ctrl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_score_expr = compute_score(df_assignments_expr, df_answers_expr)\n",
    "df_score_expr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_statistics(df, data_type, num_questions, ignore_columns):\n",
    "    average_column_name = 'average-' + data_type\n",
    "    columns = create_columns_for_data_type(data_type, num_questions)\n",
    "    selected_columns = [column for column in columns if column not in ignore_columns]\n",
    "    df[average_column_name] = df[selected_columns].mean(axis=1)\n",
    "    mean = df[average_column_name].mean()\n",
    "    std = df[average_column_name].std()\n",
    "    return mean, std, df[average_column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "score_mean_ctrl, score_std_ctrl, scores_ctrl = compute_statistics(df_score_ctrl, 'confidence', 4, ['question1-confidence'])\n",
    "score_mean_expr, score_std_expr, scores_expr = compute_statistics(df_score_expr, 'confidence', 4, ['question1-confidence'])\n",
    "print('Score statistics:')\n",
    "print('Control group: mean=%f, std=%f' % (score_mean_ctrl, score_std_ctrl))\n",
    "print('Experimental group: mean=%f, std=%f' % (score_mean_expr, score_std_expr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame({'scores_ctrl': scores_ctrl, 'scores_expr': scores_expr})\n",
    "ax_scores = df_scores.plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "axhist_scores = df_scores.plot.hist(bins=20, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "acc_mean_ctrl, acc_std_ctrl, accs_ctrl = compute_statistics(df_accuracy_ctrl, 'answer', 4, ['question1-answer'])\n",
    "acc_mean_expr, acc_std_expr, accs_expr = compute_statistics(df_accuracy_expr, 'answer', 4, ['question1-answer'])\n",
    "print('Accuracy statistics:')\n",
    "print('Control group: mean=%f, std=%f' % (acc_mean_ctrl, acc_std_ctrl))\n",
    "print('Experimental group: mean=%f, std=%f' % (acc_mean_expr, acc_std_expr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_accs = pd.DataFrame({'accs_ctrl': accs_ctrl, 'accs_expr': accs_expr})\n",
    "ax_accs = df_accs.plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "axhist_accs = df_accs.plot.hist(bins=20, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def filter_columns(columns, ignore_columns):\n",
    "    return [column for column in columns if column not in ignore_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_mixed_effect_table(df_accuracy, df_score, ignore_columns):\n",
    "    mixed_effect_columns = ['question-ref', 'group', 'participant', 'accuracy', 'score']\n",
    "    df_mixed_effect = pd.DataFrame(columns=mixed_effect_columns)\n",
    "    \n",
    "    ref_columns = create_columns_for_data_type('ref', 4) # question[i]-ref\n",
    "    answer_columns = create_columns_for_data_type('answer', 4) # question[i]-answer (accuracy)\n",
    "    confidence_columns = create_columns_for_data_type('confidence', 4) # question[i]-confidence (accuracy-confidence score)\n",
    "    \n",
    "    # Filter out some columns (e.g., data from validation question)\n",
    "    selected_ref_columns = filter_columns(ref_columns, ignore_columns)\n",
    "    selected_answer_columns = filter_columns(answer_columns, ignore_columns)\n",
    "    selected_confidence_columns = filter_columns(confidence_columns, ignore_columns)\n",
    "    \n",
    "    worker_column = ['Worker ID']\n",
    "    selected_df_accuracy = df_accuracy[selected_ref_columns + selected_answer_columns + worker_column]\n",
    "    selected_df_score = df_score[selected_ref_columns + selected_confidence_columns + worker_column]\n",
    "    \n",
    "    for i in range(selected_df_accuracy.shape[0]):\n",
    "        row_accuracy = selected_df_accuracy.iloc[i]\n",
    "        row_score = selected_df_score.iloc[i]\n",
    "        participant = row_accuracy.get('Worker ID')\n",
    "        for i in range(4):\n",
    "            if ref_columns[i] in ignore_columns:\n",
    "                continue\n",
    "            ref = row_accuracy.get(ref_columns[i])\n",
    "            group = 'experimental' if '-explanation' in ref else 'control'\n",
    "            accuracy = row_accuracy.get(answer_columns[i])\n",
    "            score = row_score.get(confidence_columns[i])\n",
    "            \n",
    "            ref_modified = ref.replace('-explanation', '')\n",
    "            row_dict = {'question-ref': ref_modified, 'group': group, 'participant': participant, 'accuracy': accuracy, 'score': score}\n",
    "            df_mixed_effect = df_mixed_effect.append(row_dict, ignore_index=True)\n",
    "    \n",
    "    return df_mixed_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_mixed_effect_ctrl = create_mixed_effect_table(df_accuracy_ctrl, df_score_ctrl, [])\n",
    "df_mixed_effect_expr = create_mixed_effect_table(df_accuracy_expr, df_score_expr, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_mixed_effect_ctrl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_mixed_effect_expr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_mixed_effect_all = df_mixed_effect_ctrl.append(df_mixed_effect_expr, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_mixed_effect_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_mixed_effect_all.to_csv('data_all.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ignore_columns_validation = ['question1-ref', 'question1-answer', 'question1-confidence']\n",
    "df_mixed_effect_3qs_ctrl = create_mixed_effect_table(df_accuracy_ctrl, df_score_ctrl, ignore_columns_validation)\n",
    "df_mixed_effect_3qs_expr = create_mixed_effect_table(df_accuracy_expr, df_score_expr, ignore_columns_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_mixed_effect_3qs_ctrl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_mixed_effect_3qs_expr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_mixed_effect_3qs_all = df_mixed_effect_3qs_ctrl.append(df_mixed_effect_3qs_expr, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_mixed_effect_3qs_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_mixed_effect_3qs_all.to_csv('data_3qs.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_mixed_effect_3qs_aligned = df_mixed_effect_3qs_all[df_mixed_effect_3qs_all['question-ref'].str.contains('-agent0')]\n",
    "# df_mixed_effect_3qs_aligned.to_csv('data_3qs_aligned.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_mixed_effect_3qs_unaligned = df_mixed_effect_3qs_all[~df_mixed_effect_3qs_all['question-ref'].str.contains('-agent0')]\n",
    "# df_mixed_effect_3qs_unaligned.to_csv('data_3qs_unaligned.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_questions_acc_ctrl = df_mixed_effect_3qs_ctrl.groupby(['question-ref'], as_index=False).mean().sort_values('accuracy')\n",
    "df_questions_acc_expr = df_mixed_effect_3qs_expr.groupby(['question-ref'], as_index=False).mean().sort_values('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_questions_acc_ctrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_questions_acc_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ax_questions_acc_ctrl = df_questions_acc_ctrl.plot.bar(x='question-ref', y='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ax_questions_acc_expr = df_questions_acc_expr.plot.bar(x='question-ref', y='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_questions_acc_expr_reindexed = df_questions_acc_expr.set_index('question-ref')\n",
    "df_questions_acc_expr_reindexed = df_questions_acc_expr_reindexed.reindex(index=df_questions_acc_ctrl['question-ref'])\n",
    "df_questions_acc_expr_reindexed = df_questions_acc_expr_reindexed.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ls_questions_acc_ctrl = df_questions_acc_ctrl['accuracy'].tolist()\n",
    "ls_questions_acc_expr = df_questions_acc_expr_reindexed['accuracy'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "questions_order = df_questions_acc_ctrl['question-ref'].tolist()\n",
    "df_questions_acc_all = pd.DataFrame({'control': ls_questions_acc_ctrl, 'experimental': ls_questions_acc_expr}, index=questions_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_questions_acc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ax_questions_acc_all = df_questions_acc_all.plot.bar(figsize=(25,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig_questions_acc_all = ax_questions_acc_all.get_figure()\n",
    "# fig_questions_acc_all.savefig('questions_acc_all.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
